{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zwq/miniconda3/envs/owod/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import datetime\n",
    "import json\n",
    "import random\n",
    "import time\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import datasets\n",
    "import util.misc as utils\n",
    "import datasets.samplers as samplers\n",
    "from datasets import build_dataset, get_coco_api_from_dataset\n",
    "from engine import evaluate, train_one_epoch\n",
    "from models import build_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: Deformable DETR training and evaluation script [-h] [--lr LR]\n",
      "                                                      [--lr_backbone_names LR_BACKBONE_NAMES [LR_BACKBONE_NAMES ...]]\n",
      "                                                      [--lr_backbone LR_BACKBONE]\n",
      "                                                      [--lr_linear_proj_names LR_LINEAR_PROJ_NAMES [LR_LINEAR_PROJ_NAMES ...]]\n",
      "                                                      [--lr_linear_proj_mult LR_LINEAR_PROJ_MULT]\n",
      "                                                      [--batch_size BATCH_SIZE]\n",
      "                                                      [--weight_decay WEIGHT_DECAY]\n",
      "                                                      [--epochs EPOCHS]\n",
      "                                                      [--lr_drop LR_DROP]\n",
      "                                                      [--lr_drop_epochs LR_DROP_EPOCHS [LR_DROP_EPOCHS ...]]\n",
      "                                                      [--clip_max_norm CLIP_MAX_NORM]\n",
      "                                                      [--sgd]\n",
      "                                                      [--with_box_refine]\n",
      "                                                      [--two_stage]\n",
      "                                                      [--frozen_weights FROZEN_WEIGHTS]\n",
      "                                                      [--backbone BACKBONE]\n",
      "                                                      [--dilation]\n",
      "                                                      [--position_embedding {sine,learned}]\n",
      "                                                      [--position_embedding_scale POSITION_EMBEDDING_SCALE]\n",
      "                                                      [--num_feature_levels NUM_FEATURE_LEVELS]\n",
      "                                                      [--enc_layers ENC_LAYERS]\n",
      "                                                      [--dec_layers DEC_LAYERS]\n",
      "                                                      [--dim_feedforward DIM_FEEDFORWARD]\n",
      "                                                      [--hidden_dim HIDDEN_DIM]\n",
      "                                                      [--dropout DROPOUT]\n",
      "                                                      [--nheads NHEADS]\n",
      "                                                      [--num_queries NUM_QUERIES]\n",
      "                                                      [--dec_n_points DEC_N_POINTS]\n",
      "                                                      [--enc_n_points ENC_N_POINTS]\n",
      "                                                      [--masks]\n",
      "                                                      [--no_aux_loss]\n",
      "                                                      [--set_cost_class SET_COST_CLASS]\n",
      "                                                      [--set_cost_bbox SET_COST_BBOX]\n",
      "                                                      [--set_cost_giou SET_COST_GIOU]\n",
      "                                                      [--mask_loss_coef MASK_LOSS_COEF]\n",
      "                                                      [--dice_loss_coef DICE_LOSS_COEF]\n",
      "                                                      [--cls_loss_coef CLS_LOSS_COEF]\n",
      "                                                      [--bbox_loss_coef BBOX_LOSS_COEF]\n",
      "                                                      [--giou_loss_coef GIOU_LOSS_COEF]\n",
      "                                                      [--focal_alpha FOCAL_ALPHA]\n",
      "                                                      [--dataset_file DATASET_FILE]\n",
      "                                                      [--coco_path COCO_PATH]\n",
      "                                                      [--coco_panoptic_path COCO_PANOPTIC_PATH]\n",
      "                                                      [--remove_difficult]\n",
      "                                                      [--output_dir OUTPUT_DIR]\n",
      "                                                      [--device DEVICE]\n",
      "                                                      [--seed SEED]\n",
      "                                                      [--resume RESUME]\n",
      "                                                      [--start_epoch N]\n",
      "                                                      [--eval]\n",
      "                                                      [--num_workers NUM_WORKERS]\n",
      "                                                      [--cache_mode]\n",
      "Deformable DETR training and evaluation script: error: ambiguous option: --f=/home/zwq/.local/share/jupyter/runtime/kernel-v2-57279s4CIrLr7GBW0.json could match --frozen_weights, --focal_alpha\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
     ]
    }
   ],
   "source": [
    "def get_args_parser():\n",
    "    parser = argparse.ArgumentParser('Deformable DETR Detector', add_help=False)\n",
    "    parser.add_argument('--lr', default=2e-4, type=float)\n",
    "    parser.add_argument('--lr_backbone_names', default=[\"backbone.0\"], type=str, nargs='+')\n",
    "    parser.add_argument('--lr_backbone', default=2e-5, type=float)\n",
    "    parser.add_argument('--lr_linear_proj_names', default=['reference_points', 'sampling_offsets'], type=str, nargs='+')\n",
    "    parser.add_argument('--lr_linear_proj_mult', default=0.1, type=float)\n",
    "    parser.add_argument('--batch_size', default=2, type=int)\n",
    "    parser.add_argument('--weight_decay', default=1e-4, type=float)\n",
    "    parser.add_argument('--epochs', default=50, type=int)\n",
    "    parser.add_argument('--lr_drop', default=40, type=int)\n",
    "    parser.add_argument('--lr_drop_epochs', default=None, type=int, nargs='+')\n",
    "    parser.add_argument('--clip_max_norm', default=0.1, type=float,\n",
    "                        help='gradient clipping max norm')\n",
    "\n",
    "\n",
    "    parser.add_argument('--sgd', action='store_true')\n",
    "\n",
    "    # Variants of Deformable DETR\n",
    "    parser.add_argument('--with_box_refine', default=False, action='store_true')\n",
    "    parser.add_argument('--two_stage', default=False, action='store_true')\n",
    "\n",
    "    # Model parameters\n",
    "    parser.add_argument('--frozen_weights', type=str, default=None,\n",
    "                        help=\"Path to the pretrained model. If set, only the mask head will be trained\")\n",
    "\n",
    "    # * Backbone\n",
    "    parser.add_argument('--backbone', default='resnet50', type=str,\n",
    "                        help=\"Name of the convolutional backbone to use\")\n",
    "    parser.add_argument('--dilation', action='store_true',\n",
    "                        help=\"If true, we replace stride with dilation in the last convolutional block (DC5)\")\n",
    "    parser.add_argument('--position_embedding', default='sine', type=str, choices=('sine', 'learned'),\n",
    "                        help=\"Type of positional embedding to use on top of the image features\")\n",
    "    parser.add_argument('--position_embedding_scale', default=2 * np.pi, type=float,\n",
    "                        help=\"position / size * scale\")\n",
    "    parser.add_argument('--num_feature_levels', default=4, type=int, help='number of feature levels')\n",
    "\n",
    "    # * Transformer\n",
    "    parser.add_argument('--enc_layers', default=6, type=int,\n",
    "                        help=\"Number of encoding layers in the transformer\")\n",
    "    parser.add_argument('--dec_layers', default=6, type=int,\n",
    "                        help=\"Number of decoding layers in the transformer\")\n",
    "    parser.add_argument('--dim_feedforward', default=1024, type=int,\n",
    "                        help=\"Intermediate size of the feedforward layers in the transformer blocks\")\n",
    "    parser.add_argument('--hidden_dim', default=256, type=int,\n",
    "                        help=\"Size of the embeddings (dimension of the transformer)\")\n",
    "    parser.add_argument('--dropout', default=0.1, type=float,\n",
    "                        help=\"Dropout applied in the transformer\")\n",
    "    parser.add_argument('--nheads', default=8, type=int,\n",
    "                        help=\"Number of attention heads inside the transformer's attentions\")\n",
    "    parser.add_argument('--num_queries', default=300, type=int,\n",
    "                        help=\"Number of query slots\")\n",
    "    parser.add_argument('--dec_n_points', default=4, type=int)\n",
    "    parser.add_argument('--enc_n_points', default=4, type=int)\n",
    "\n",
    "    # * Segmentation\n",
    "    parser.add_argument('--masks', action='store_true',\n",
    "                        help=\"Train segmentation head if the flag is provided\")\n",
    "\n",
    "    # Loss\n",
    "    parser.add_argument('--no_aux_loss', dest='aux_loss', action='store_true',\n",
    "                        help=\"Disables auxiliary decoding losses (loss at each layer)\")\n",
    "\n",
    "    # * Matcher\n",
    "    parser.add_argument('--set_cost_class', default=2, type=float,\n",
    "                        help=\"Class coefficient in the matching cost\")\n",
    "    parser.add_argument('--set_cost_bbox', default=5, type=float,\n",
    "                        help=\"L1 box coefficient in the matching cost\")\n",
    "    parser.add_argument('--set_cost_giou', default=2, type=float,\n",
    "                        help=\"giou box coefficient in the matching cost\")\n",
    "\n",
    "    # * Loss coefficients\n",
    "    parser.add_argument('--mask_loss_coef', default=1, type=float)\n",
    "    parser.add_argument('--dice_loss_coef', default=1, type=float)\n",
    "    parser.add_argument('--cls_loss_coef', default=2, type=float)\n",
    "    parser.add_argument('--bbox_loss_coef', default=5, type=float)\n",
    "    parser.add_argument('--giou_loss_coef', default=2, type=float)\n",
    "    parser.add_argument('--focal_alpha', default=0.25, type=float)\n",
    "\n",
    "    # dataset parameters\n",
    "    parser.add_argument('--dataset_file', default='coco')\n",
    "    parser.add_argument('--coco_path', default='./data/coco', type=str)\n",
    "    parser.add_argument('--coco_panoptic_path', type=str)\n",
    "    parser.add_argument('--remove_difficult', action='store_true')\n",
    "\n",
    "    parser.add_argument('--output_dir', default='',\n",
    "                        help='path where to save, empty for no saving')\n",
    "    parser.add_argument('--device', default='cuda',\n",
    "                        help='device to use for training / testing')\n",
    "    parser.add_argument('--seed', default=42, type=int)\n",
    "    parser.add_argument('--resume', default='r50_deformable_detr-checkpoint.pth', help='resume from checkpoint')\n",
    "    parser.add_argument('--start_epoch', default=0, type=int, metavar='N',\n",
    "                        help='start epoch')\n",
    "    parser.add_argument('--eval', action='store_true')\n",
    "    parser.add_argument('--num_workers', default=0, type=int)\n",
    "    parser.add_argument('--cache_mode', default=False, action='store_true', help='whether to cache images on memory')\n",
    "\n",
    "    return parser\n",
    "\n",
    "\n",
    "parser = argparse.ArgumentParser('Deformable DETR training and evaluation script', parents=[get_args_parser()])\n",
    "args = parser.parse_args()\n",
    "if args.output_dir:\n",
    "    Path(args.output_dir).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "\n",
    "utils.init_distributed_mode(args)\n",
    "print(\"git:\\n  {}\\n\".format(utils.get_sha()))\n",
    "\n",
    "if args.frozen_weights is not None:\n",
    "    assert args.masks, \"Frozen training is meant for segmentation only\"\n",
    "print(args)\n",
    "\n",
    "device = torch.device(args.device)\n",
    "\n",
    "# fix the seed for reproducibility\n",
    "seed = args.seed + utils.get_rank()\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "\n",
    "model, criterion, postprocessors = build_model(args)\n",
    "model.to(device)\n",
    "\n",
    "model_without_ddp = model\n",
    "n_parameters = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print('number of params:', n_parameters)\n",
    "\n",
    "dataset_train = build_dataset(image_set='train', args=args)\n",
    "dataset_val = build_dataset(image_set='val', args=args)\n",
    "\n",
    "if args.distributed:\n",
    "    if args.cache_mode:\n",
    "        sampler_train = samplers.NodeDistributedSampler(dataset_train)\n",
    "        sampler_val = samplers.NodeDistributedSampler(dataset_val, shuffle=False)\n",
    "    else:\n",
    "        sampler_train = samplers.DistributedSampler(dataset_train)\n",
    "        sampler_val = samplers.DistributedSampler(dataset_val, shuffle=False)\n",
    "else:\n",
    "    sampler_train = torch.utils.data.RandomSampler(dataset_train)\n",
    "    sampler_val = torch.utils.data.SequentialSampler(dataset_val)\n",
    "\n",
    "batch_sampler_train = torch.utils.data.BatchSampler(\n",
    "    sampler_train, args.batch_size, drop_last=True)\n",
    "\n",
    "data_loader_train = DataLoader(dataset_train, batch_sampler=batch_sampler_train,\n",
    "                                collate_fn=utils.collate_fn, num_workers=args.num_workers,\n",
    "                                pin_memory=True)\n",
    "data_loader_val = DataLoader(dataset_val, args.batch_size, sampler=sampler_val,\n",
    "                                drop_last=False, collate_fn=utils.collate_fn, num_workers=args.num_workers,\n",
    "                                pin_memory=True)\n",
    "\n",
    "# lr_backbone_names = [\"backbone.0\", \"backbone.neck\", \"input_proj\", \"transformer.encoder\"]\n",
    "def match_name_keywords(n, name_keywords):\n",
    "    out = False\n",
    "    for b in name_keywords:\n",
    "        if b in n:\n",
    "            out = True\n",
    "            break\n",
    "    return out\n",
    "\n",
    "for n, p in model_without_ddp.named_parameters():\n",
    "    print(n)\n",
    "\n",
    "param_dicts = [\n",
    "    {\n",
    "        \"params\":\n",
    "            [p for n, p in model_without_ddp.named_parameters()\n",
    "                if not match_name_keywords(n, args.lr_backbone_names) and not match_name_keywords(n, args.lr_linear_proj_names) and p.requires_grad],\n",
    "        \"lr\": args.lr,\n",
    "    },\n",
    "    {\n",
    "        \"params\": [p for n, p in model_without_ddp.named_parameters() if match_name_keywords(n, args.lr_backbone_names) and p.requires_grad],\n",
    "        \"lr\": args.lr_backbone,\n",
    "    },\n",
    "    {\n",
    "        \"params\": [p for n, p in model_without_ddp.named_parameters() if match_name_keywords(n, args.lr_linear_proj_names) and p.requires_grad],\n",
    "        \"lr\": args.lr * args.lr_linear_proj_mult,\n",
    "    }\n",
    "]\n",
    "if args.sgd:\n",
    "    optimizer = torch.optim.SGD(param_dicts, lr=args.lr, momentum=0.9,\n",
    "                                weight_decay=args.weight_decay)\n",
    "else:\n",
    "    optimizer = torch.optim.AdamW(param_dicts, lr=args.lr,\n",
    "                                    weight_decay=args.weight_decay)\n",
    "lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, args.lr_drop)\n",
    "\n",
    "if args.distributed:\n",
    "    model = torch.nn.parallel.DistributedDataParallel(model, device_ids=[args.gpu])\n",
    "    model_without_ddp = model.module\n",
    "\n",
    "if args.dataset_file == \"coco_panoptic\":\n",
    "    # We also evaluate AP during panoptic training, on original coco DS\n",
    "    coco_val = datasets.coco.build(\"val\", args)\n",
    "    base_ds = get_coco_api_from_dataset(coco_val)\n",
    "else:\n",
    "    base_ds = get_coco_api_from_dataset(dataset_val)\n",
    "\n",
    "if args.frozen_weights is not None:\n",
    "    checkpoint = torch.load(args.frozen_weights, map_location='cpu')\n",
    "    model_without_ddp.detr.load_state_dict(checkpoint['model'])\n",
    "\n",
    "output_dir = Path(args.output_dir)\n",
    "if args.resume:\n",
    "    if args.resume.startswith('https'):\n",
    "        checkpoint = torch.hub.load_state_dict_from_url(\n",
    "            args.resume, map_location='cpu', check_hash=True)\n",
    "    else:\n",
    "        checkpoint = torch.load(args.resume, map_location='cpu')\n",
    "    missing_keys, unexpected_keys = model_without_ddp.load_state_dict(checkpoint['model'], strict=False)\n",
    "    unexpected_keys = [k for k in unexpected_keys if not (k.endswith('total_params') or k.endswith('total_ops'))]\n",
    "    if len(missing_keys) > 0:\n",
    "        print('Missing Keys: {}'.format(missing_keys))\n",
    "    if len(unexpected_keys) > 0:\n",
    "        print('Unexpected Keys: {}'.format(unexpected_keys))\n",
    "    if not args.eval and 'optimizer' in checkpoint and 'lr_scheduler' in checkpoint and 'epoch' in checkpoint:\n",
    "        import copy\n",
    "        p_groups = copy.deepcopy(optimizer.param_groups)\n",
    "        optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "        for pg, pg_old in zip(optimizer.param_groups, p_groups):\n",
    "            pg['lr'] = pg_old['lr']\n",
    "            pg['initial_lr'] = pg_old['initial_lr']\n",
    "        print(optimizer.param_groups)\n",
    "        lr_scheduler.load_state_dict(checkpoint['lr_scheduler'])\n",
    "        # todo: this is a hack for doing experiment that resume from checkpoint and also modify lr scheduler (e.g., decrease lr in advance).\n",
    "        args.override_resumed_lr_drop = True\n",
    "        if args.override_resumed_lr_drop:\n",
    "            print('Warning: (hack) args.override_resumed_lr_drop is set to True, so args.lr_drop would override lr_drop in resumed lr_scheduler.')\n",
    "            lr_scheduler.step_size = args.lr_drop\n",
    "            lr_scheduler.base_lrs = list(map(lambda group: group['initial_lr'], optimizer.param_groups))\n",
    "        lr_scheduler.step(lr_scheduler.last_epoch)\n",
    "        args.start_epoch = checkpoint['epoch'] + 1\n",
    "    # check the resumed model\n",
    "    if not args.eval:\n",
    "        test_stats, coco_evaluator = evaluate(\n",
    "            model, criterion, postprocessors, data_loader_val, base_ds, device, args.output_dir\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if args.eval:\n",
    "    print(base_ds)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "owod",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
